[
  {
    "objectID": "index.html#начало-работы",
    "href": "index.html#начало-работы",
    "title": "Аккуратное моделирование с помощью R",
    "section": "Начало работы",
    "text": "Начало работы\nВ данном документе проведен анализ подразделения ресторана #proСчастье\nДля анализа были использован отчета из ПО iiko - “Отчет о продажах за период”\nСтоит отметить, что данные отчеты были предвариельно обработаны и преведены к единому формату."
  },
  {
    "objectID": "intro.html#меню-все-сейчас-хорошо",
    "href": "intro.html#меню-все-сейчас-хорошо",
    "title": "1  Отчет о продажах за период",
    "section": "1.1 Меню “Все сейчас хорошо”",
    "text": "1.1 Меню “Все сейчас хорошо”\nНачнем с анализа меню ресторана “Все сейчас хорошо”, так как на данный момент это является акутальным для нас.\nНа первом шаге мы загрущзим данные и оставим только те позиции, которые относятся к меню “ВСХ”. Все позиции меню данного ресторана обозначены “ВСХ <наименования позиции меню>”.\nПосмотрим на структуру данных\n\n\n\n\n  \n\n\n\nДанные представлены с 2021-01-05 по 2023-01-31. В них содержится 20 колонок. Не все они требуют пояснения.\n\n\n\n\n\n\nЗаметка\n\n\n\nМы детально рассмотрим только те колонки, которые требуют внимания\n\n\n\ncode - код наменклатуры (SKU) в iiko\ngroup - группа первого уровня в iiko\nfood_name - наименоание позиции в меню\ncount - количество реализованнх SKU в конкретный день\nrevenue_no_sale - выручка без скидок в конкретный день по конкретному SKU\nsum_sale - сумма скидок в рублях\nrevenue - выручка за вычитом скидок\nfood_cost - фудкост рассчитанный в iiko\nallowance - наценка\ndiscount_rate - размер скидки. Данная переменая нужна, чтобы рассчитать реальный фудкост, где не будут учтены наценки за обслуживания\nrevenue_end - очищенная выручка от наценок\nfood_cost_end - расчитанный фудкост\n\nПосморим какие группы меню содержатся в наших данных\n\n\n\n\n  \n\n\n\nДобавим в наши данные еще одну колонку, которая будет содержать тип группы: “Еда” или “Бар” и посмотрим на соотношение количества и суммы продаж по двум этим параметрам.\n\n\n\n\n  \n\n\n\nМы видим, что с 2021-01-05 по 2023-01-31 еды было продано больше, чем позиций бара. Но по сумме выручки – цифры сопоставимые.\n\nДинамика продаж\nПосмотрим на динамику продаж по этим двум группам.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nМы видим, что динамика продаж отрицательная. Сложно сделать окончательные выводы, по имеющимся данным, так как у нас их недостаточно. Возможно отрицательная динамика – это всего лишь сезонность.\nПри этом стоит отметить, что продажи бара менее вариабельны, чем позиции по кухне.\n\nДинамика foodcost по группам в меню\n\n\n\n\n\n\n\n\n\n\n\nПопулярные позиции меню\nТеперь мы можем посмотреть на те позиции, которые наиболее часто покупают в “ВСХ”. Нас больше интересует не сама группа, а конкретное SKU.\nСначала посмотрим на позиции, которые относятся к категории “Еда”. Выделим ТОП 50\n\n\n\n\n\n\n\n\n\nТеперь выделим TOP 50 позиций бара:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nОбратите внимание\n\n\n\nДанная ифонрмация представлена за весь период работы бара “Все сейчас хорошо”\n\n\nДавайте посмотрим на продажи позиций из меню за последние 3 мес. Таким образмо, мы полчим только те SKU, которые есть в меню.\n\n\n\n\n\n\n\n\n\nПосмотрим на выручкку по этим же параметрам\n\n\n\n\n\n\n\n\n\nПредтавим данные в таблице.\n\n\n\n\n  \n\n\n\nСудя по данным, на мой взгляд, позиции, которые выходят в ТОП, связаны с детскими лагерями.\nОтдельно рассмотрим позиции бара “ВСХ” за теже три месяца.\n\n\n\n\n\n\n\n\n\nПосмотрим на выручкку по этим же параметрам\n\n\n\n\n\n\n\n\n\nПредтавим данные в таблице."
  },
  {
    "objectID": "intro.html#меню-ресторана-proсчастье",
    "href": "intro.html#меню-ресторана-proсчастье",
    "title": "1  Отчет о продажах за период",
    "section": "1.2 Меню ресторана #proСчастье",
    "text": "1.2 Меню ресторана #proСчастье\n\nБар\nВ данном разделе мы проанализируем продажи барного меню ресторана #proСчастье. У нас есть данные за период с 2017-01-01 по 2023-01-31. Нам инетресен промежуток с июня 2022 по 2023-01-31.\nНа первом этапе посмотрим, какие группы меню наиболее востребованы. Рассмотрим весь период, имеющийся в данных.\nКоличество проданных групп SKU по годам\n\n\n\n\n\n\n\n\n\nВыручка проданных групп SKU по годам\n\n\n\n\n\n\n\n\n\nСредний foodcost проданных групп SKU по годам\n\n\n\n\n\n\n\n\n\nДавайте посмотри на динамику продаж по группам в течение 2022 года.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВ целом мы видим отрицательную динамику с теченим времени как по выручке, так и по количеству продаваемых позиций бара.\nДавайте посмотрим какую долю занимает каждая группа в продажах.\n\n\n\n\n\n\n\n\n\n90% продаж приходится на позиции выделенные цветом.\nОтдельно посмотрим на долю продаж только за 2022 год\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% продаж приходится на позиции выделенные цветом.\n\n\n\n\n\n\nОбратите внимание\n\n\n\nДалее на графиках выделенное цветом соответсвует 95% продаж\n\n\n\n\nДетализация по группам меню бара\n\nПивоНапиткиКоктейли алкКофеВискиВодкаЧайВино игристоеВино белоеВина по бокаламКоньякКоктейли б/аВино красноеСвежевыжатые сокиЛимонадыБиттерТекилаЛикерыБарВино розовоеВермут"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Аккуратное моделирование с помощью R",
    "section": "",
    "text": "Welcome to Tidy Modeling with R! This book is a guide to using a collection of software in the R programming language for model building called tidymodels, and it has two main goals:\nIn Chapter @ref(software-modeling), we outline a taxonomy for models and highlight what good software for modeling is like. The ideas and syntax of the tidyverse, which we introduce (or review) in Chapter @ref(tidyverse), are the basis for the tidymodels approach to these challenges of methodology and practice. Chapter @ref(base-r) provides a quick tour of conventional base R modeling functions and summarizes the unmet needs in that area.\nAfter that, this book is separated into parts, starting with the basics of modeling with tidy data principles. Chapters @ref(ames) through @ref(performance) introduces an example data set on house prices and demonstrates how to use the fundamental tidymodels packages: recipes, parsnip), workflows, yardstick, and others.\nThe next part of the book moves forward with more details on the process of creating an effective model. Chapters @ref(resampling) through @ref(workflow-sets) focus on creating good estimates of performance as well as tuning model hyperparameters.\nFinally, the last section of this book, Chapters @ref(dimensionality) through @ref(inferential), covers other important topics for model building. We discuss more advanced feature engineering approaches like dimensionality reduction and encoding high cardinality predictors, as well as how to answer questions about why a model makes certain predictions and when to trust your model predictions.\nWe do not assume that readers have extensive experience in model building and statistics. Some statistical knowledge is required, such as random sampling, variance, correlation, basic linear regression, and other topics that are usually found in a basic undergraduate statistics or data analysis course. We do assume that the reader is at least slightly familiar with dplyr, ggplot2, and the %>% “pipe” operator in R, and is interested in applying these tools to modeling. For users who don’t yet have this background R knowledge, we recommend books such as R for Data Science by Wickham and Grolemund (2016). Investigating and analyzing data are an important part of any model process.\nThis book is not intended to be a comprehensive reference on modeling techniques; we suggest other resources to learn more about the statistical methods themselves. For general background on the most common type of model, the linear model, we suggest Fox (2008). For predictive models, Kuhn and Johnson (2013) and Kuhn and Johnson (2020) are good resources. For machine learning methods, Goodfellow, Bengio, and Courville (2016) is an excellent (but formal) source of information. In some cases, we do describe the models we use in some detail, but in a way that is less mathematical, and hopefully more intuitive."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Аккуратное моделирование с помощью R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\n\n\nWe are so thankful for the contributions, help, and perspectives of people who have supported us in this project. There are several we would like to thank in particular.\nWe would like to thank our RStudio colleagues on the tidymodels team (Davis Vaughan, Hannah Frick, Emil Hvitfeldt, and Simon Couch) as well as the rest of our coworkers on the RStudio open source team. Thank you to Desirée De Leon for the site design of the online work. We would also like to thank our technical reviewers, Chelsea Parlett-Pelleriti and Dan Simpson, for their detailed, insightful feedback that substantively improved this book, as well as our editors, Nicole Tache and Rita Fernando, for their perspective and guidance during the process of writing and publishing.\nThis book was written in the open, and multiple people contributed via pull requests or issues. Special thanks goes to the forty-four people who contributed via GitHub pull requests (in alphabetical order by username): Bikram Halder (@BikramHalder), @DCharIAA, @DavZim, Emil Hvitfeldt (@EmilHvitfeldt), Fgazzelloni (@Fgazzelloni), John W Pickering (@JohnPickering), @MikeJohnPage, Philippe Massicotte (@PMassicotte), Y. Yu (@PursuitOfDataScience), Raymond R. Balise, PhD (@RaymondBalise), Rob Wiederstein (@RobWiederstein), shitao (@Shitao5), @arisp99, Brad Hill (@bradisbrad), Bryce Roney (@bryceroney), Cedric Batailler (@cedricbatailler), Ildikó Czeller (@czeildi), David Kane (@davidkane9), Emilio (@emilopezcano), Hannah Frick (@hfrick), Hlynur (@hlynurhallgrims), Howard Baek (@howardbaek), Jae Yeon Kim (@jaeyk), Jonathan D. Trattner (@jdtrat), Jeffrey Girard (@jmgirard), Jon Harmon (@jonthegeek), Joseph B. Rickert (@joseph-rickert), Maximilian Rohde (@maxdrohde), Michael Grund (@michaelgrund), Mine Cetinkaya-Rundel (@mine-cetinkaya-rundel), Mohammed Hamdy (@mmhamdy), @nattalides, Riaz Hedayati (@riazhedayati), Scott (@scottyd22), Simon P. Couch (@simonpcouch), Simon Schölzel (@simonschoe), Simon Sayz (@tagasimon), @thrkng, Tanner Stauss (@tmstauss), Tony ElHabr (@tonyelhabr), Dmitry Zotikov (@x1o), Xiaochi (@xiaochi-liu), Zach Bogart (@zachbogart), Zeki Akyol (@zekiakyol)."
  },
  {
    "objectID": "index.html#using-code-examples",
    "href": "index.html#using-code-examples",
    "title": "Аккуратное моделирование с помощью R",
    "section": "Using Code Examples",
    "text": "Using Code Examples\n\n\n\nThis book was written with RStudio using bookdown. The website is hosted via Netlify, and automatically built after every push by GitHub Actions. The complete source is available on GitHub. We generated all plots in this book using ggplot2 and its black and white theme (theme_bw()).\nThis version of the book was built with R version 4.2.2 (2022-10-31), pandoc version 2.19.2, and the following packages: applicable (0.1.0, CRAN), av (0.8.3, CRAN), baguette (1.0.0, CRAN), beans (0.1.0, CRAN), bestNormalize (1.8.3, CRAN), bookdown (0.32, CRAN), broom (1.0.3, CRAN), censored (0.1.1, CRAN), corrplot (0.92, CRAN), corrr (0.4.4, CRAN), Cubist (0.4.2, CRAN), DALEXtra (2.2.1, CRAN), dials (1.1.0, CRAN), dimRed (NA, NA), discrim (1.0.0, CRAN), doMC (1.3.8, CRAN), dplyr (1.1.0, CRAN), earth (5.3.2, CRAN), embed (1.0.0, CRAN), fastICA (NA, NA), finetune (1.0.1, CRAN), forcats (1.0.0, CRAN), ggforce (0.4.1, CRAN), ggplot2 (3.4.0, CRAN), glmnet (4.1-6, CRAN), gridExtra (2.3, CRAN), infer (1.0.4, CRAN), kableExtra (1.3.4, CRAN), kernlab (0.9-32, CRAN), kknn (1.3.1, CRAN), klaR (NA, NA), knitr (1.42, CRAN), learntidymodels (NA, NA), lime (NA, NA), lme4 (1.1-31, CRAN), lubridate (1.9.1, CRAN), mda (NA, NA), mixOmics (NA, NA), modeldata (1.1.0, CRAN), multilevelmod (1.0.0, CRAN), nlme (3.1-162, CRAN), nnet (7.3-18, CRAN), parsnip (1.0.3, CRAN), patchwork (1.1.2, CRAN), pillar (1.8.1, CRAN), poissonreg (1.0.1, CRAN), prettyunits (1.1.1, CRAN), probably (0.1.0, CRAN), pscl (NA, NA), purrr (1.0.1, CRAN), ranger (0.14.1, CRAN), recipes (1.0.4, CRAN), rlang (1.0.6, CRAN), rmarkdown (2.20, CRAN), rpart (4.1.19, CRAN), rsample (1.1.1, CRAN), rstanarm (2.21.3, CRAN), rules (1.0.1, CRAN), sessioninfo (1.2.2, CRAN), stacks (1.0.1, CRAN), stringr (1.5.0, CRAN), svglite (2.1.1, CRAN), text2vec (NA, NA), textrecipes (1.0.2, CRAN), themis (1.0.0, CRAN), tibble (3.1.8, CRAN), tidymodels (1.0.0, CRAN), tidyposterior (1.0.0, CRAN), tidyverse (1.3.2, CRAN), tune (1.0.1, CRAN), uwot (0.1.14, CRAN), workflows (1.1.2, CRAN), workflowsets (1.0.0, CRAN), xgboost (1.7.3.1, CRAN), and yardstick (1.1.0, CRAN).\n\n\n\n\nFox, J. 2008. Applied Regression Analysis and Generalized Linear Models. Second. Thousand Oaks, CA: Sage.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT Press.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\n———. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "(PART*) Introduction",
    "section": "",
    "text": "Software for modeling\nModels are mathematical tools that can describe a system and capture relationships in the data given to them. Models can be used for various purposes, including predicting future events, determining if there is a difference between several groups, aiding map-based visualization, discovering novel patterns in the data that could be further investigated, and more. The utility of a model hinges on its ability to be reductive, or to reduce complex relationships to simpler terms. The primary influences in the data can be captured mathematically in a useful way, such as in a relationship that can be expressed as an equation.\nSince the beginning of the twenty-first century, mathematical models have become ubiquitous in our daily lives, in both obvious and subtle ways. A typical day for many people might involve checking the weather to see when might be a good time to walk the dog, ordering a product from a website, typing a text message to a friend and having it autocorrected, and checking email. In each of these instances, there is a good chance that some type of model was involved. In some cases, the contribution of the model might be easily perceived (“You might also be interested in purchasing product X”) while in other cases, the impact could be the absence of something (e.g., spam email). Models are used to choose clothing that a customer might like, to identify a molecule that should be evaluated as a drug candidate, and might even be the mechanism that a nefarious company uses to avoid the discovery of cars that over-pollute. For better or worse, models are here to stay.\nThis book focuses largely on software. It is obviously critical that software produces the correct relationships to represent the data. For the most part, determining mathematical correctness is possible, but the reliable creation of appropriate models requires more. In this chapter, we outline considerations for building or choosing modeling software, the purposes of models, and where modeling sits in the broader data analysis process."
  },
  {
    "objectID": "intro.html#fundamentals-for-modeling-software",
    "href": "intro.html#fundamentals-for-modeling-software",
    "title": "1  Software for modeling",
    "section": "1.1 Fundamentals for Modeling Software",
    "text": "1.1 Fundamentals for Modeling Software\nIt is important that the modeling software you use is easy to operate properly. The user interface should not be so poorly designed that the user would not know that they used it inappropriately. For example, Baggerly and Coombes (2009) report myriad problems in the data analyses from a high profile computational biology publication. One of the issues was related to how the users were required to add the names of the model inputs. The software user interface made it easy to offset the column names of the data from the actual data columns. This resulted in the wrong genes being identified as important for treating cancer patients and eventually contributed to the termination of several clinical trials (Carlson 2012).\nIf we need high quality models, software must facilitate proper usage. Abrams (2003) describes an interesting principle to guide us:\n\nThe Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks.\n\nData analysis and modeling software should espouse this idea.\nSecond, modeling software should promote good scientific methodology. When working with complex predictive models, it can be easy to unknowingly commit errors related to logical fallacies or inappropriate assumptions. Many machine learning models are so adept at discovering patterns that they can effortlessly find empirical patterns in the data that fail to reproduce later. Some of methodological errors are insidious in that the issue can go undetected until a later time when new data that contain the true result are obtained.\n\nAs our models have become more powerful and complex, it has also become easier to commit latent errors.\n\nThis same principle also applies to programming. Whenever possible, the software should be able to protect users from committing mistakes. Software should make it easy for users to do the right thing.\nThese two aspects of model development – ease of proper use and good methodological practice – are crucial. Since tools for creating models are easily accessible and models can have such a profound impact, many more people are creating them. In terms of technical expertise and training, creators’ backgrounds will vary. It is important that their tools be robust to the user’s experience. Tools should be powerful enough to create high-performance models, but, on the other hand, should be easy to use appropriately. This book describes a suite of software for modeling that has been designed with these characteristics in mind.\nThe software is based on the R programming language (R Core Team 2014). R has been designed especially for data analysis and modeling. It is an implementation of the S language (with lexical scoping rules adapted from Scheme and Lisp) which was created in the 1970s to\n\n“turn ideas into software, quickly and faithfully” (Chambers 1998)\n\nR is open source and free. It is a powerful programming language that can be used for many different purposes but specializes in data analysis, modeling, visualization, and machine learning. R is easily extensible; it has a vast ecosystem of packages, mostly user-contributed modules that focus on a specific theme, such as modeling, visualization, and so on.\nOne collection of packages is called the tidyverse (Wickham et al. 2019). The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. Several of these design philosophies are directly informed by the aspects of software for modeling described in this chapter. If you’ve never used the tidyverse packages, Chapter @ref(tidyverse) contains a review of basic concepts. Within the tidyverse, the subset of packages specifically focused on modeling are referred to as the tidymodels packages. This book is a practical guide for conducting modeling using the tidyverse and tidymodels packages. It shows how to use a set of packages, each with its own specific purpose, together to create high-quality models."
  },
  {
    "objectID": "intro.html#model-types",
    "href": "intro.html#model-types",
    "title": "1  Software for modeling",
    "section": "1.2 Types of Models",
    "text": "1.2 Types of Models\nBefore proceeding, let’s describe a taxonomy for types of models, grouped by purpose. This taxonomy informs both how a model is used and many aspects of how the model may be created or evaluated. While this list is not exhaustive, most models fall into at least one of these categories:\n\nDescriptive models\nThe purpose of a descriptive model is to describe or illustrate characteristics of some data. The analysis might have no other purpose than to visually emphasize some trend or artifact in the data.\nFor example, large scale measurements of RNA have been possible for some time using microarrays. Early laboratory methods placed a biological sample on a small microchip. Very small locations on the chip can measure a signal based on the abundance of a specific RNA sequence. The chip would contain thousands (or more) outcomes, each a quantification of the RNA related to a biological process. However, there could be quality issues on the chip that might lead to poor results. For example, a fingerprint accidentally left on a portion of the chip could cause inaccurate measurements when scanned.\nAn early method for evaluating such issues were probe-level models, or PLMs (Bolstad 2004). A statistical model would be created that accounted for the known differences in the data, such as the chip, the RNA sequence, the type of sequence, and so on. If there were other, unknown factors in the data, these effects would be captured in the model residuals. When the residuals were plotted by their location on the chip, a good quality chip would show no patterns. When a problem did occur, some sort of spatial pattern would be discernible. Often the type of pattern would suggest the underlying issue (e.g., a fingerprint) and a possible solution (wipe off the chip and rescan, repeat the sample, etc.). Figure @ref(fig:software-descr-examples)(a) shows an application of this method for two microarrays taken from Gentleman et al. (2005). The images show two different color values; areas that are darker are where the signal intensity was larger than the model expects while the lighter color shows lower than expected values. The left-hand panel demonstrates a fairly random pattern while the right-hand panel exhibits an undesirable artifact in the middle of the chip.\n\n\n\n\n\nTwo examples of how descriptive models can be used to illustrate specific patterns\n\n\n\n\nAnother example of a descriptive model is the locally estimated scatterplot smoothing model, more commonly known as LOESS (Cleveland 1979). Here, a smooth and flexible regression model is fit to a data set, usually with a single independent variable, and the fitted regression line is used to elucidate some trend in the data. These types of smoothers are used to discover potential ways to represent a variable in a model. This is demonstrated in Figure @ref(fig:software-descr-examples)(b) where a nonlinear trend is illuminated by the flexible smoother. From this plot, it is clear that there is a highly nonlinear relationship between the sale price of a house and its latitude.\n\n\nInferential models\nThe goal of an inferential model is to produce a decision for a research question or to explore a specific hypothesis, similar to how statistical tests are used.1 An inferential model starts with a predefined conjecture or idea about a population and produces a statistical conclusion such as an interval estimate or the rejection of a hypothesis.\nFor example, the goal of a clinical trial might be to provide confirmation that a new therapy does a better job in prolonging life than an alternative, such as an existing therapy or no treatment at all. If the clinical endpoint related to survival of a patient, the null hypothesis might be that the new treatment has an equal or lower median survival time, with the alternative hypothesis being that the new therapy has higher median survival. If this trial were evaluated using traditional null hypothesis significance testing via modeling, the significance testing would produce a p-value using some pre-defined methodology based on a set of assumptions for the data. Small values for the p-value in the model results would indicate there is evidence that the new therapy helps patients live longer. Large values for the p-value in the model results would conclude there is a failure to show such a difference; this lack of evidence could be due to a number of reasons, including the therapy not working.\nWhat are the important aspects of this type of analysis? Inferential modeling techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. Generally, to compute such a quantity, formal probabilistic assumptions must be made about the data and the underlying processes that generated the data. The quality of the statistical modeling results are highly dependent on these pre-defined assumptions as well as how much the observed data appear to agree with them. The most critical factors here are theoretical: “If my data were independent and the residuals follow distribution X, then test statistic Y can be used to produce a p-value. Otherwise, the resulting p-value might be inaccurate.”\n\nOne aspect of inferential analyses is that there tends to be a delayed feedback loop in understanding how well the data match the model assumptions. In our clinical trial example, if statistical (and clinical) significance indicate that the new therapy should be available for patients to use, it still may be years before it is used in the field and enough data are generated for an independent assessment of whether the original statistical analysis led to the appropriate decision.\n\n\n\nPredictive models\nSometimes data are modeled to produce the most accurate prediction possible for new data. Here, the primary goal is that the predicted values have the highest possible fidelity to the true value of the new data.\nA simple example would be for a book buyer to predict how many copies of a particular book should be shipped to their store for the next month. An over-prediction wastes space and money due to excess books. If the prediction is smaller than it should be, there is opportunity loss and less profit.\nFor this type of model, the problem type is one of estimation rather than inference. For example, the buyer is usually not concerned with a question such as “Will I sell more than 100 copies of book X next month?” but rather “How many copies of book X will customers purchase next month?” Also, depending on the context, there may not be any interest in why the predicted value is X. In other words, there is more interest in the value itself than in evaluating a formal hypothesis related to the data. The prediction can also include measures of uncertainty. In the case of the book buyer, providing a forecasting error may be helpful in deciding how many books to purchase. It can also serve as a metric to gauge how well the prediction method worked.\nWhat are the most important factors affecting predictive models? There are many different ways that a predictive model can be created, so the important factors depend on how the model was developed.2\nA mechanistic model could be derived using first principles to produce a model equation that depends on assumptions. For example, when predicting the amount of a drug that is in a person’s body at a certain time, some formal assumptions are made on how the drug is administered, absorbed, metabolized, and eliminated. Based on this, a set of differential equations can be used to derive a specific model equation. Data are used to estimate the unknown parameters of this equation so that predictions can be generated. Like inferential models, mechanistic predictive models greatly depend on the assumptions that define their model equations. However, unlike inferential models, it is easy to make data-driven statements about how well the model performs based on how well it predicts the existing data. Here the feedback loop for the modeling practitioner is much faster than it would be for a hypothesis test.\nEmpirically driven models are created with more vague assumptions. These models tend to fall into the machine learning category. A good example is the K-nearest neighbor (KNN) model. Given a set of reference data, a new sample is predicted by using the values of the K most similar data in the reference set. For example, if a book buyer needs a prediction for a new book, historical data from existing books may be available. A 5-nearest neighbor model would estimate the number of the new books to purchase based on the sales numbers of the five books that are most similar to the new one (for some definition of “similar”). This model is defined only by the structure of the prediction (the average of five similar books). No theoretical or probabilistic assumptions are made about the sales numbers or the variables that are used to define similarity. In fact, the primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data. If the structure of this type of model was a good choice, the predictions would be close to the actual values."
  },
  {
    "objectID": "intro.html#connections-between-types-of-models",
    "href": "intro.html#connections-between-types-of-models",
    "title": "1  Software for modeling",
    "section": "1.3 Connections Between Types of Models",
    "text": "1.3 Connections Between Types of Models\n\nNote that we have defined the type of a model by how it is used, rather than its mathematical qualities.\n\nAn ordinary linear regression model might fall into any of these three classes of model, depending on how it is used:\n\nA descriptive smoother, similar to LOESS, called restricted smoothing splines (Durrleman and Simon 1989) can be used to describe trends in data using ordinary linear regression with specialized terms.\nAn analysis of variance (ANOVA) model is a popular method for producing the p-values used for inference. ANOVA models are a special case of linear regression.\nIf a simple linear regression model produces accurate predictions, it can be used as a predictive model.\n\nThere are many examples of predictive models that cannot (or at least should not) be used for inference. Even if probabilistic assumptions were made for the data, the nature of the K-nearest neighbors model, for example, makes the math required for inference intractable.\nThere is an additional connection between the types of models. While the primary purpose of descriptive and inferential models might not be related to prediction, the predictive capacity of the model should not be ignored. For example, logistic regression is a popular model for data in which the outcome is qualitative with two possible values. It can model how variables are related to the probability of the outcomes. When used inferentially, an abundance of attention is paid to the statistical qualities of the model. For example, analysts tend to strongly focus on the selection of independent variables contained in the model. Many iterations of model building may be used to determine a minimal subset of independent variables that have a “statistically significant” relationship to the outcome variable. This is usually achieved when all of the p-values for the independent variables are below a certain value (e.g., 0.05). From here, the analyst may focus on making qualitative statements about the relative influence that the variables have on the outcome (e.g., “There is a statistically significant relationship between age and the odds of heart disease.”).\nHowever, this approach can be dangerous when statistical significance is used as the only measure of model quality. It is possible that this statistically optimized model has poor model accuracy, or it performs poorly on some other measure of predictive capacity. While the model might not be used for prediction, how much should inferences be trusted from a model that has significant p-values but dismal accuracy? Predictive performance tends to be related to how close the model’s fitted values are to the observed data.\n\nIf a model has limited fidelity to the data, the inferences generated by the model should be highly suspect. In other words, statistical significance may not be sufficient proof that a model is appropriate.\n\nThis may seem intuitively obvious, but it is often ignored in real-world data analysis."
  },
  {
    "objectID": "intro.html#model-terminology",
    "href": "intro.html#model-terminology",
    "title": "1  Software for modeling",
    "section": "1.4 Some Terminology",
    "text": "1.4 Some Terminology\nBefore proceeding, we will outline additional terminology related to modeling and data. These descriptions are intended to be helpful as you read this book, but they are not exhaustive.\nFirst, many models can be categorized as being supervised or unsupervised. Unsupervised models are those that learn patterns, clusters, or other characteristics of the data but lack an outcome, i.e., a dependent variable. Principal component analysis (PCA), clustering, and autoencoders are examples of unsupervised models; they are used to understand relationships between variables or sets of variables without an explicit relationship between predictors and an outcome. Supervised models are those that have an outcome variable. Linear regression, neural networks, and numerous other methodologies fall into this category.\nWithin supervised models, there are two main sub-categories:\n\nRegression predicts a numeric outcome.\nClassification predicts an outcome that is an ordered or unordered set of qualitative values.\n\nThese are imperfect definitions and do not account for all possible model types. In Chapter @ref(models), we refer to this characteristic of supervised techniques as the model mode.\nDifferent variables can have different roles, especially in a supervised modeling analysis. Outcomes (otherwise known as the labels, endpoints, or dependent variables) are the value being predicted in supervised models. The independent variables, which are the substrate for making predictions of the outcome, are also referred to as predictors, features, or covariates (depending on the context). The terms outcomes and predictors are used most frequently in this book.\nIn terms of the data or variables themselves, whether used for supervised or unsupervised models, as predictors or outcomes, the two main categories are quantitative and qualitative. Examples of the former are real numbers like 3.14159 and integers like 42. Qualitative values, also known as nominal data, are those that represent some sort of discrete state that cannot be naturally placed on a numeric scale, like “red”, “green”, and “blue”."
  },
  {
    "objectID": "intro.html#model-phases",
    "href": "intro.html#model-phases",
    "title": "1  Software for modeling",
    "section": "1.5 How Does Modeling Fit into the Data Analysis Process?",
    "text": "1.5 How Does Modeling Fit into the Data Analysis Process?\nIn what circumstances are models created? Are there steps that precede such an undertaking? Is model creation the first step in data analysis?\n\nThere are a few critical phases of data analysis that always come before modeling.\n\nFirst, there is the chronically underestimated process of cleaning the data. No matter the circumstances, you should investigate the data to make sure that they are applicable to your project goals, accurate, and appropriate. These steps can easily take more time than the rest of the data analysis process (depending on the circumstances).\nData cleaning can also overlap with the second phase of understanding the data, often referred to as exploratory data analysis (EDA). EDA brings to light how the different variables are related to one another, their distributions, typical ranges, and other attributes. A good question to ask at this phase is, “How did I come by these data?” This question can help you understand how the data at hand have been sampled or filtered and if these operations were appropriate. For example, when merging database tables, a join may go awry that could accidentally eliminate one or more subpopulations. Another good idea is to ask if the data are relevant. For example, to predict whether patients have Alzheimer’s disease, it would be unwise to have a data set containing subjects with the disease and a random sample of healthy adults from the general population. Given the progressive nature of the disease, the model may simply predict who are the oldest patients.\nFinally, before starting a data analysis process, there should be clear expectations of the model’s goal and how performance (and success) will be judged. At least one performance metric should be identified with realistic goals of what can be achieved. Common statistical metrics, discussed in more detail in Chapter @ref(performance), are classification accuracy, true and false positive rates, root mean squared error, and so on. The relative benefits and drawbacks of these metrics should be weighed. It is also important that the metric be germane; alignment with the broader data analysis goals is critical.\nThe process of investigating the data may not be simple. Wickham and Grolemund (2016) contains an excellent illustration of the general data analysis process, reproduced in Figure @ref(fig:software-data-science-model). Data ingestion and cleaning/tidying are shown as the initial steps. When the analytical steps for understanding commence, they are a heuristic process; we cannot pre-determine how long they may take. The cycle of transformation, modeling, and visualization often requires multiple iterations.\n\n\n\n\n\nThe data science process (from R for Data Science, used with permission)\n\n\n\n\nThis iterative process is especially true for modeling. Figure @ref(fig:software-modeling-process) emulates the typical path to determining an appropriate model. The general phases are:\n\nExploratory data analysis (EDA): Initially there is a back and forth between numerical analysis and data visualization (represented in Figure @ref(fig:software-data-science-model)) where different discoveries lead to more questions and data analysis side-quests to gain more understanding.\nFeature engineering: The understanding gained from EDA results in the creation of specific model terms that make it easier to accurately model the observed data. This can include complex methodologies (e.g., PCA) or simpler features (using the ratio of two predictors). Chapter @ref(recipes) focuses entirely on this important step.\nModel tuning and selection (large circles with alternating segments): A variety of models are generated and their performance is compared. Some models require parameter tuning in which some structural parameters must be specified or optimized. The alternating segments within the circles signify the repeated data splitting used during resampling (see Chapter @ref(resampling)).\nModel evaluation: During this phase of model development, we assess the model’s performance metrics, examine residual plots, and conduct other EDA-like analyses to understand how well the models work. In some cases, formal between-model comparisons (Chapter @ref(compare)) help you understand whether any differences in models are within the experimental noise.\n\n\n\n\n\n\nA schematic for the typical modeling process\n\n\n\n\nAfter an initial sequence of these tasks, more understanding is gained regarding which models are superior as well as which data subpopulations are not being effectively estimated. This leads to additional EDA and feature engineering, another round of modeling, and so on. Once the data analysis goals are achieved, typically the last steps are to finalize, document, and communicate the model. For predictive models, it is common at the end to validate the model on an additional set of data reserved for this specific purpose.\nAs an example, Kuhn and Johnson (2020) use data to model the daily ridership of Chicago’s public train system using predictors such as the date, the previous ridership results, the weather, and other factors. Table @ref(tab:inner-monologue) shows an approximation of these authors’ hypothetical inner monologue when analyzing these data and eventually selecting a model with sufficient performance.\n\n\n\n\nHypothetical inner monologue of a model developer.\n \n  \n    Thoughts \n    Activity \n  \n \n\n  \n    The daily ridership values between stations are extremely correlated. \n    EDA \n  \n  \n    Weekday and weekend ridership look very different. \n    EDA \n  \n  \n    One day in the summer of 2010 has an abnormally large number of riders. \n    EDA \n  \n  \n    Which stations had the lowest daily ridership values? \n    EDA \n  \n  \n    Dates should at least be encoded as day-of-the-week, and year. \n    Feature Engineering \n  \n  \n    Maybe PCA could be used on the correlated predictors to make it easier for the models to use them. \n    Feature Engineering \n  \n  \n    Hourly weather records should probably be summarized into daily measurements. \n    Feature Engineering \n  \n  \n    Let’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree. \n    Model Fitting \n  \n  \n    How many neighbors should be used? \n    Model Tuning \n  \n  \n    Should we run a lot of boosting iterations or just a few? \n    Model Tuning \n  \n  \n    How many neighbors seemed to be optimal for these data? \n    Model Tuning \n  \n  \n    Which models have the lowest root mean squared errors? \n    Model Evaluation \n  \n  \n    Which days were poorly predicted? \n    EDA \n  \n  \n    Variable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models. \n    Model Evaluation \n  \n  \n    It seems like we should focus on a lot of boosting iterations for that model. \n    Model Evaluation \n  \n  \n    We need to encode holiday features to improve predictions on (and around) those dates. \n    Feature Engineering \n  \n  \n    Let’s drop KNN from the model list. \n    Model Evaluation"
  },
  {
    "objectID": "intro.html#software-summary",
    "href": "intro.html#software-summary",
    "title": "1  Software for modeling",
    "section": "1.6 Chapter Summary",
    "text": "1.6 Chapter Summary\nThis chapter focused on how models describe relationships in data, and different types of models such as descriptive models, inferential models, and predictive models. The predictive capacity of a model can be used to evaluate it, even when its main goal is not prediction. Modeling itself sits within the broader data analysis process, and exploratory data analysis is a key part of building high-quality models.\n\n\n\n\nAbrams, B. 2003. “The Pit of Success.” https://blogs.msdn.microsoft.com/brada/2003/10/02/the-pit-of-success/.\n\n\nBaggerly, K, and K Coombes. 2009. “Deriving Chemosensitivity from Cell Lines: Forensic Bioinformatics and Reproducible Research in High-Throughput Biology.” The Annals of Applied Statistics 3 (4): 1309–34.\n\n\nBolstad, B. 2004. Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.\n\n\nBreiman, L. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231.\n\n\nCarlson, B. 2012. “Putting Oncology Patients at Risk.” Biotechnology Healthcare 9 (3): 17–21.\n\n\nChambers, J. 1998. Programming with Data: A Guide to the S Language. Berlin, Heidelberg: Springer-Verlag.\n\n\nCleveland, W. 1979. “Robust Locally Weighted Regression and Smoothing Scatterplots.” Journal of the American Statistical Association 74 (368): 829–36.\n\n\nDurrleman, S, and R Simon. 1989. “Flexible Regression Models with Cubic Splines.” Statistics in Medicine 8 (5): 551–61.\n\n\nGentleman, R, V Carey, W Huber, R Irizarry, and S Dudoit. 2005. Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Berlin, Heidelberg: Springer-Verlag.\n\n\nKuhn, M, and K Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nR Core Team. 2014. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/.\n\n\nShmueli, G. 2010. “To Explain or to Predict?” Statistical Science 25 (3): 289–310.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43).\n\n\nWickham, H, and G Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc."
  },
  {
    "objectID": "02-tidyverse.html#tidyverse-principles",
    "href": "02-tidyverse.html#tidyverse-principles",
    "title": "2  A Tidyverse Primer",
    "section": "2.1 Tidyverse Principles",
    "text": "2.1 Tidyverse Principles\nThe full set of strategies and tactics for writing R code in the tidyverse style can be found at the website https://design.tidyverse.org. Here we can briefly describe several of the general tidyverse design principles, their motivation, and how we think about modeling as an application of these principles.\n\n2.1.1 Design for humans\nThe tidyverse focuses on designing R packages and functions that can be easily understood and used by a broad range of people. Both historically and today, a substantial percentage of R users are not people who create software or tools but instead people who create analyses or models. As such, R users do not typically have (or need) computer science backgrounds, and many are not interested in writing their own R packages.\nFor this reason, it is critical that R code be easy to work with to accomplish your goals. Documentation, training, accessibility, and other factors play an important part in achieving this. However, if the syntax itself is difficult for people to easily comprehend, documentation is a poor solution. The software itself must be intuitive.\nTo contrast the tidyverse approach with more traditional R semantics, consider sorting a data frame. Data frames can represent different types of data in each column, and multiple values in each row. Using only the core language, we can sort a data frame using one or more columns by reordering the rows via R’s subscripting rules in conjunction with order(); you cannot successfully use a function you might be tempted to try in such a situation because of its name, sort(). To sort the mtcars data by two of its columns, the call might look like:\n\n\nCode\nmtcars[order(mtcars$gear, mtcars$mpg), ]\n\n\nWhile very computationally efficient, it would be difficult to argue that this is an intuitive user interface. In dplyr by contrast, the tidyverse function arrange() takes a set of variable names as input arguments directly:\n\n\nCode\nlibrary(dplyr)\narrange(.data = mtcars, gear, mpg)\n\n\n\nThe variable names used here are “unquoted”; many traditional R functions require a character string to specify variables, but tidyverse functions take unquoted names or selector functions. The selectors allow for one or more readable rules that are applied to the column names. For example, ends_with(\"t\") would select the drat and wt columns of the mtcars data frame.\n\nAdditionally, naming is crucial. If you were new to R and were writing data analysis or modeling code involving linear algebra, you might be stymied when searching for a function that computes the matrix inverse. Using apropos(\"inv\") yields no candidates. It turns out that the base R function for this task is solve(), for solving systems of linear equations. For a matrix X, you would use solve(X) to invert X (with no vector for the right-hand side of the equation). This is only documented in the description of one of the arguments in the help file. In essence, you need to know the name of the solution to be able to find the solution.\nThe tidyverse approach is to use function names that are descriptive and explicit over those that are short and implicit. There is a focus on verbs (e.g., fit, arrange, etc.) for general methods. Verb-noun pairs are particularly effective; consider invert_matrix() as a hypothetical function name. In the context of modeling, it is also important to avoid highly technical jargon, such as Greek letters or obscure terms in terms. Names should be as self-documenting as possible.\nWhen there are similar functions in a package, function names are designed to be optimized for tab-completion. For example, the glue package has a collection of functions starting with a common prefix (glue_) that enables users to quickly find the function they are looking for.\n\n\n2.1.2 Reuse existing data structures\nWhenever possible, functions should avoid returning a novel data structure. If the results are conducive to an existing data structure, it should be used. This reduces the cognitive load when using software; no additional syntax or methods are required.\nThe data frame is the preferred data structure in tidyverse and tidymodels packages, because its structure is a good fit for such a broad swath of data science tasks. Specifically, the tidyverse and tidymodels favor the tibble, a modern reimagining of R’s data frame that we describe in the next section on example tidyverse syntax.\nAs an example, the rsample package can be used to create resamples of a data set, such as cross-validation or the bootstrap (described in Chapter @ref(resampling)). The resampling functions return a tibble with a column called splits of objects that define the resampled data sets. Three bootstrap samples of a data set might look like:\n\n\nCode\nboot_samp <- rsample::bootstraps(mtcars, times = 3)\nboot_samp\n\n\n\n\n  \n\n\n\nCode\nclass(boot_samp)\n\n\n[1] \"bootstraps\" \"rset\"       \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWith this approach, vector-based functions can be used with these columns, such as vapply() or purrr::map().1 This boot_samp object has multiple classes but inherits methods for data frames (\"data.frame\") and tibbles (\"tbl_df\"). Additionally, new columns can be added to the results without affecting the class of the data. This is much easier and more versatile for users to work with than a completely new object type that does not make its data structure obvious.\nOne downside to relying on common data structures is the potential loss of computational performance. In some situations, data can be encoded in specialized formats that are more efficient representations of the data. For example:\n\nIn computational chemistry, the structure-data file format (SDF) is a tool to take chemical structures and encode them in a format that is computationally efficient to work with.\nData that have a large number of values that are the same (such as zeros for binary data) can be stored in a sparse matrix format. This format can reduce the size of the data as well as enable more efficient computational techniques.\n\nThese formats are advantageous when the problem is well scoped and the potential data processing methods are both well defined and suited to such a format.2 However, once such constraints are violated, specialized data formats are less useful. For example, if we perform a transformation of the data that converts the data into fractional numbers, the output is no longer sparse; the sparse matrix representation is helpful for one specific algorithmic step in modeling, but this is often not true before or after that specific step.\n\nA specialized data structure is not flexible enough for an entire modeling workflow in the way that a common data structure is.\n\nOne important feature in the tibble produced by rsample is that the splits column is a list. In this instance, each element of the list has the same type of object: an rsplit object that contains the information about which rows of mtcars belong in the bootstrap sample. List columns can be very useful in data analysis and, as will be seen throughout this book, are important to tidymodels.\n\n\n2.1.3 Design for the pipe and functional programming\nThe magrittr pipe operator (%>%) is a tool for chaining together a sequence of R functions.3 To demonstrate, consider the following commands that sort a data frame and then retain the first 10 rows:\n\n\nCode\nsmall_mtcars <- arrange(mtcars, gear)\nsmall_mtcars <- slice(small_mtcars, 1:10)\n\n# or more compactly: \nsmall_mtcars <- slice(arrange(mtcars, gear), 1:10)\n\n\nThe pipe operator substitutes the value of the left-hand side of the operator as the first argument to the right-hand side, so we can implement the same result as before with:\n\n\nCode\nsmall_mtcars <- \n  mtcars %>% \n  arrange(gear) %>% \n  slice(1:10)\n\n\nThe piped version of this sequence is more readable; this readability increases as more operations are added to a sequence. This approach to programming works in this example because all of the functions we used return the same data structure (a data frame) that is then the first argument to the next function. This is by design. When possible, create functions that can be incorporated into a pipeline of operations.\nIf you have used ggplot2, this is not unlike the layering of plot components into a ggplot object with the + operator. To make a scatter plot with a regression line, the initial ggplot() call is augmented with two additional operations:\n\n\nCode\nlibrary(ggplot2)\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() + \n  geom_smooth(method = lm)\n\n\nWhile similar to the dplyr pipeline, note that the first argument to this pipeline is a data set (mtcars) and that each function call returns a ggplot object. Not all pipelines need to keep the returned values (plot objects) the same as the initial value (a data frame). Using the pipe operator with dplyr operations has acclimated many R users to expect to return a data frame when pipelines are used; as shown with ggplot2, this does not need to be the case. Pipelines are incredibly useful in modeling workflows but modeling pipelines can return, instead of a data frame, objects such as model components.\nR has excellent tools for creating, changing, and operating on functions, making it a great language for functional programming. This approach can replace iterative loops in many situations, such as when a function returns a value without other side effects.4\nLet’s look at an example. Suppose you are interested in the logarithm of the ratio of the fuel efficiency to the car weight. To those new to R and/or coming from other programming languages, a loop might seem like a good option:\n\n\nCode\nn <- nrow(mtcars)\nratios <- rep(NA_real_, n)\nfor (car in 1:n) {\n  ratios[car] <- log(mtcars$mpg[car]/mtcars$wt[car])\n}\nhead(ratios)\n\n\n[1] 2.081348 1.988470 2.285193 1.895564 1.693052 1.654643\n\n\nThose with more experience in R may know that there is a much simpler and faster vectorized version that can be computed by:\n\n\nCode\nratios <- log(mtcars$mpg/mtcars$wt)\n\n\nHowever, in many real-world cases, the element-wise operation of interest is too complex for a vectorized solution. In such a case, a good approach is to write a function to do the computations. When we design for functional programming, it is important that the output depends only on the inputs and that the function has no side effects. Violations of these ideas in the following function are shown with comments:\n\n\nCode\ncompute_log_ratio <- function(mpg, wt) {\n  log_base <- getOption(\"log_base\", default = exp(1)) # gets external data\n  results <- log(mpg/wt, base = log_base)\n  print(mean(results))                                # prints to the console\n  done <<- TRUE                                       # sets external data\n  results\n}\n\n\nA better version would be:\n\n\nCode\ncompute_log_ratio <- function(mpg, wt, log_base = exp(1)) {\n  log(mpg/wt, base = log_base)\n}\n\n\nThe purrr package contains tools for functional programming. Let’s focus on the map() family of functions, which operates on vectors and always returns the same type of output. The most basic function, map(), always returns a list and uses the basic syntax of map(vector, function). For example, to take the square root of our data, we could:\n\n\nCode\nmap(head(mtcars$mpg, 3), sqrt)\n\n\n[[1]]\n[1] 4.582576\n\n[[2]]\n[1] 4.582576\n\n[[3]]\n[1] 4.774935\n\n\nThere are specialized variants of map() that return values when we know or expect that the function will generate one of the basic vector types. For example, since the square root returns a double-precision number:\n\n\nCode\nmap_dbl(head(mtcars$mpg, 3), sqrt)\n\n\n[1] 4.582576 4.582576 4.774935\n\n\nThere are also mapping functions that operate across multiple vectors:\n\n\nCode\nlog_ratios <- map2_dbl(mtcars$mpg, mtcars$wt, compute_log_ratio)\nhead(log_ratios)\n\n\n[1] 2.081348 1.988470 2.285193 1.895564 1.693052 1.654643\n\n\nThe map() functions also allow for temporary, anonymous functions defined using the tilde character. The argument values are .x and .y for map2():\n\n\nCode\nmap2_dbl(mtcars$mpg, mtcars$wt, ~ log(.x/.y)) %>% \n  head()\n\n\n[1] 2.081348 1.988470 2.285193 1.895564 1.693052 1.654643\n\n\nThese examples have been trivial but, in later sections, will be applied to more complex problems.\n\nFor functional programming in tidy modeling, functions should be defined so that functions like map() can be used for iterative computations."
  },
  {
    "objectID": "02-tidyverse.html#examples-of-tidyverse-syntax",
    "href": "02-tidyverse.html#examples-of-tidyverse-syntax",
    "title": "2  A Tidyverse Primer",
    "section": "2.2 Examples of Tidyverse Syntax",
    "text": "2.2 Examples of Tidyverse Syntax\nLet’s begin our discussion of tidyverse syntax by exploring more deeply what a tibble is, and how tibbles work. Tibbles have slightly different rules than basic data frames in R. For example, tibbles naturally work with column names that are not syntactically valid variable names:\n\n\nCode\n# Wants valid names:\ndata.frame(`variable 1` = 1:2, two = 3:4)\n\n\n\n\n  \n\n\n\nCode\n# But can be coerced to use them with an extra option:\ndf <- data.frame(`variable 1` = 1:2, two = 3:4, check.names = FALSE)\ndf\n\n\n\n\n  \n\n\n\nCode\n# But tibbles just work:\ntbbl <- tibble(`variable 1` = 1:2, two = 3:4)\ntbbl\n\n\n\n\n  \n\n\n\nStandard data frames enable partial matching of arguments so that code using only a portion of the column names still works. Tibbles prevent this from happening since it can lead to accidental errors.\n\n\nCode\ndf$tw\n\n\n[1] 3 4\n\n\nCode\ntbbl$tw\n\n\nWarning: Unknown or uninitialised column: `tw`.\n\n\nNULL\n\n\nTibbles also prevent one of the most common R errors: dropping dimensions. If a standard data frame subsets the columns down to a single column, the object is converted to a vector. Tibbles never do this:\n\n\nCode\ndf[, \"two\"]\n\n\n[1] 3 4\n\n\nCode\ntbbl[, \"two\"]\n\n\n\n\n  \n\n\n\nThere are other advantages to using tibbles instead of data frames, such as better printing and more.5\nTo demonstrate some syntax, let’s use tidyverse functions to read in data that could be used in modeling. The data set comes from the city of Chicago’s data portal and contains daily ridership data for the city’s elevated train stations. The data set has columns for:\n\nthe station identifier (numeric)\nthe station name (character)\nthe date (character in mm/dd/yyyy format)\nthe day of the week (character)\nthe number of riders (numeric)\n\nOur tidyverse pipeline will conduct the following tasks, in order:\n\nUse the tidyverse package readr to read the data from the source website and convert them into a tibble. To do this, the read_csv() function can determine the type of data by reading an initial number of rows. Alternatively, if the column names and types are already known, a column specification can be created in R and passed to read_csv().\nFilter the data to eliminate a few columns that are not needed (such as the station ID) and change the column stationname to station. The function select() is used for this. When filtering, use either the column names or a dplyr selector function. When selecting names, a new variable name can be declared using the argument format new_name = old_name.\nConvert the date field to the R date format using the mdy() function from the lubridate package. We also convert the ridership numbers to thousands. Both of these computations are executed using the dplyr::mutate() function.\nUse the maximum number of rides for each station and day combination. This mitigates the issue of a small number of days that have more than one record of ridership numbers at certain stations. We group the ridership data by station and day, and then summarize within each of the 1999 unique combinations with the maximum statistic.\n\nThe tidyverse code for these steps is:\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\n\nurl <- \"https://data.cityofchicago.org/api/views/5neh-572f/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\n\nall_stations <- \n  # Step 1: Read in the data.\n  read_csv(url) %>% \n  # Step 2: filter columns and rename stationname\n  dplyr::select(station = stationname, date, rides) %>% \n  # Step 3: Convert the character date field to a date encoding.\n  # Also, put the data in units of 1K rides\n  mutate(date = mdy(date), rides = rides / 1000) %>% \n  # Step 4: Summarize the multiple records using the maximum.\n  group_by(date, station) %>% \n  summarize(rides = max(rides), .groups = \"drop\")\n\n\nThis pipeline of operations illustrates why the tidyverse is popular. A series of data manipulations is used that have simple and easy to understand functions for each transformation; the series is bundled in a streamlined, readable way. The focus is on how the user interacts with the software. This approach enables more people to learn R and achieve their analysis goals, and adopting these same principles for modeling in R has the same benefits."
  },
  {
    "objectID": "02-tidyverse.html#chapter-summary",
    "href": "02-tidyverse.html#chapter-summary",
    "title": "2  A Tidyverse Primer",
    "section": "2.3 Chapter Summary",
    "text": "2.3 Chapter Summary\nThis chapter introduced the tidyverse, with a focus on applications for modeling and how tidyverse design principles inform the tidymodels framework. Think of the tidymodels framework as applying tidyverse principles to the domain of building models. We described differences in conventions between the tidyverse and base R, and introduced two important components of the tidyverse system, tibbles and the pipe operator %>%. Data cleaning and processing can feel mundane at times, but these tasks are important for modeling in the real world; we illustrated how to use tibbles, the pipe, and tidyverse functions in an example data import and processing exercise.\n\n\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43).\n\n\nWickham, H, and G Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc."
  },
  {
    "objectID": "03-base-r.html#an-example",
    "href": "03-base-r.html#an-example",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "3.1 An Example",
    "text": "3.1 An Example\nTo demonstrate some fundamentals for modeling in base R, let’s use experimental data from McDonald (2009), by way of Mangiafico (2015), on the relationship between the ambient temperature and the rate of cricket chirps per minute. Data were collected for two species: O. exclamationis and O. niveus. The data are contained in a data frame called crickets with a total of 31 data points. These data are shown in Figure @ref(fig:cricket-plot) using the following ggplot2 code.\n\n\nCode\nlibrary(tidyverse)\n\ndata(crickets, package = \"modeldata\")\nnames(crickets)\n\n# Plot the temperature on the x-axis, the chirp rate on the y-axis. The plot\n# elements will be colored differently for each species:\nggplot(crickets, \n       aes(x = temp, y = rate, color = species, pch = species, lty = species)) + \n  # Plot points for each data point and color by species\n  geom_point(size = 2) + \n  # Show a simple linear model fit created separately for each species:\n  geom_smooth(method = lm, se = FALSE, alpha = 0.5) + \n  scale_color_brewer(palette = \"Paired\") +\n  labs(x = \"Temperature (C)\", y = \"Chirp Rate (per minute)\")\n\n\n\n\n[1] \"species\" \"temp\"    \"rate\"   \n\n\n\n\n\nRelationship between chirp rate and temperature for two different species of crickets\n\n\n\n\nThe data exhibit fairly linear trends for each species. For a given temperature, O. exclamationis appears to chirp more per minute than the other species. For an inferential model, the researchers might have specified the following null hypotheses prior to seeing the data:\n\nTemperature has no effect on the chirp rate.\nThere are no differences between the species’ chirp rate.\n\nThere may be some scientific or practical value in predicting the chirp rate but in this example we will focus on inference.\nTo fit an ordinary linear model in R, the lm() function is commonly used. The important arguments to this function are a model formula and a data frame that contains the data. The formula is symbolic. For example, the simple formula:\nrate ~ temp\nspecifies that the chirp rate is the outcome (since it is on the left-hand side of the tilde ~) and that the temperature value is the predictor.1 Suppose the data contained the time of day in which the measurements were obtained in a column called time. The formula:\nrate ~ temp + time\nwould not add the time and temperature values together. This formula would symbolically represent that temperature and time should be added as separate main effects to the model. A main effect is a model term that contains a single predictor variable.\nThere are no time measurements in these data but the species can be added to the model in the same way:\nrate ~ temp + species\nSpecies is not a quantitative variable; in the data frame, it is represented as a factor column with levels \"O. exclamationis\" and \"O. niveus\". The vast majority of model functions cannot operate on nonnumeric data. For species, the model needs to encode the species data in a numeric format. The most common approach is to use indicator variables (also known as dummy variables) in place of the original qualitative values. In this instance, since species has two possible values, the model formula will automatically encode this column as numeric by adding a new column that has a value of zero when the species is \"O. exclamationis\" and a value of one when the data correspond to \"O. niveus\". The underlying formula machinery automatically converts these values for the data set used to create the model, as well as for any new data points (for example, when the model is used for prediction).\n\nSuppose there were five species instead of two. The model formula, in this case, would create four binary columns that are binary indicators for four of the species. The reference level of the factor (i.e., the first level) is always left out of the predictor set. The idea is that, if you know the values of the four indicator variables, the value of the species can be determined. We discuss binary indicator variables in more detail in Section @ref(dummies).\n\nThe model formula rate ~ temp + species creates a model with different y-intercepts for each species; the slopes of the regression lines could be different for each species as well. To accommodate this structure, an interaction term can be added to the model. This can be specified in a few different ways, and the most basic uses the colon:\nrate ~ temp + species + temp:species\n\n# A shortcut can be used to expand all interactions containing\n# interactions with two variables:\nrate ~ (temp + species)^2\n\n# Another shortcut to expand factors to include all possible\n# interactions (equivalent for this example):\nrate ~ temp * species\nIn addition to the convenience of automatically creating indicator variables, the formula offers a few other niceties:\n\nIn-line functions can be used in the formula. For example, to use the natural log of the temperature, we can create the formula rate ~ log(temp). Since the formula is symbolic by default, literal math can also be applied to the predictors using the identity function I(). To use Fahrenheit units, the formula could be rate ~ I( (temp * 9/5) + 32 ) to convert from Celsius.\nR has many functions that are useful inside of formulas. For example, poly(x, 3) adds linear, quadratic, and cubic terms for x to the model as main effects. The splines package also has several functions to create nonlinear spline terms in the formula.\nFor data sets where there are many predictors, the period shortcut is available. The period represents main effects for all of the columns that are not on the left-hand side of the tilde. Using ~ (.)^3 would add main effects as well as all two- and three-variable interactions to the model.\n\nReturning to our chirping crickets, let’s use a two-way interaction model. In this book, we use the suffix _fit for R objects that are fitted models.\n\n\nCode\ninteraction_fit <-  lm(rate ~ (temp + species)^2, data = crickets) \n\n# To print a short summary of the model:\ninteraction_fit\n\n\n\nCall:\nlm(formula = rate ~ (temp + species)^2, data = crickets)\n\nCoefficients:\n          (Intercept)                   temp       speciesO. niveus  \n              -11.041                  3.751                 -4.348  \ntemp:speciesO. niveus  \n               -0.234  \n\n\nThis output is a little hard to read. For the species indicator variables, R mashes the variable name (species) together with the factor level (O. niveus) with no delimiter.\nBefore going into any inferential results for this model, the fit should be assessed using diagnostic plots. We can use the plot() method for lm objects. This method produces a set of four plots for the object, each showing different aspects of the fit, as shown in Figure @ref(fig:interaction-plots).\n\n\nCode\n# Place two plots next to one another:\npar(mfrow = c(1, 2))\n\n# Show residuals vs predicted values:\nplot(interaction_fit, which = 1)\n\n# A normal quantile plot on the residuals:\nplot(interaction_fit, which = 2)\n\n\n\n\n\n\n\nResidual diagnostic plots for the linear model with interactions, which appear reasonable enough to conduct inferential analysis\n\n\n\n\n\nWhen it comes to the technical details of evaluating expressions, R is lazy (as opposed to eager). This means that model fitting functions typically compute the minimum possible quantities at the last possible moment. For example, if you are interested in the coefficient table for each model term, this is not automatically computed with the model but is instead computed via the summary() method.\n\nOur next order of business with the crickets is to assess if the inclusion of the interaction term is necessary. The most appropriate approach for this model is to recompute the model without the interaction term and use the anova() method.\n\n\nCode\n# Fit a reduced model:\nmain_effect_fit <-  lm(rate ~ temp + species, data = crickets) \n\n# Compare the two:\nanova(main_effect_fit, interaction_fit)\n\n\n\n\n  \n\n\n\nThis statistical test generates a p-value of 0.25425. This implies that there is a lack of evidence against the null hypothesis that the interaction term is not needed by the model. For this reason, we will conduct further analysis on the model without the interaction.\nResidual plots should be reassessed to make sure that our theoretical assumptions are valid enough to trust the p-values produced by the model (plots not shown here but spoiler alert: they are).\nWe can use the summary() method to inspect the coefficients, standard errors, and p-values of each model term:\n\n\nCode\nsummary(main_effect_fit)\n\n\n\nCall:\nlm(formula = rate ~ temp + species, data = crickets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0128 -1.1296 -0.3912  0.9650  3.7800 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -7.21091    2.55094  -2.827  0.00858 ** \ntemp               3.60275    0.09729  37.032  < 2e-16 ***\nspeciesO. niveus -10.06529    0.73526 -13.689 6.27e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.786 on 28 degrees of freedom\nMultiple R-squared:  0.9896,    Adjusted R-squared:  0.9888 \nF-statistic:  1331 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\nThe chirp rate for each species increases by 3.6 chirps as the temperature increases by a single degree. This term shows strong statistical significance as evidenced by the p-value. The species term has a value of -10.07. This indicates that, across all temperature values, O. niveus has a chirp rate that is about 10 fewer chirps per minute than O. exclamationis. Similar to the temperature term, the species effect is associated with a very small p-value.\nThe only issue in this analysis is the intercept value. It indicates that at 0° C, there are negative chirps per minute for both species. While this doesn’t make sense, the data only go as low as 17.2° C and interpreting the model at 0° C would be an extrapolation. This would be a bad idea. That being said, the model fit is good within the applicable range of the temperature values; the conclusions should be limited to the observed temperature range.\nIf we needed to estimate the chirp rate at a temperature that was not observed in the experiment, we could use the predict() method. It takes the model object and a data frame of new values for prediction. For example, the model estimates the chirp rate for O. exclamationis for temperatures between 15° C and 20° C can be computed via:\n\n\nCode\nnew_values <- data.frame(species = \"O. exclamationis\", temp = 15:20)\npredict(main_effect_fit, new_values)\n\n\n       1        2        3        4        5        6 \n46.83039 50.43314 54.03589 57.63865 61.24140 64.84415 \n\n\n\nNote that the non-numeric value of species is passed to the predict method, as opposed to the numeric, binary indicator variable.\n\nWhile this analysis has obviously not been an exhaustive demonstration of R’s modeling capabilities, it does highlight some major features important for the rest of this book:\n\nThe language has an expressive syntax for specifying model terms for both simple and quite complex models.\nThe R formula method has many conveniences for modeling that are also applied to new data when predictions are generated.\nThere are numerous helper functions (e.g., anova(), summary() and predict()) that you can use to conduct specific calculations after the fitted model is created.\n\nFinally, as previously mentioned, this framework was first published in 1992. Most of these ideas and methods were developed in that period but have remained remarkably relevant to this day. It highlights that the S language and, by extension R, has been designed for data analysis since its inception."
  },
  {
    "objectID": "03-base-r.html#formula",
    "href": "03-base-r.html#formula",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "3.2 What Does the R Formula Do?",
    "text": "3.2 What Does the R Formula Do?\nThe R model formula is used by many modeling packages. It usually serves multiple purposes:\n\nThe formula defines the columns that the model uses.\nThe standard R machinery uses the formula to encode the columns into an appropriate format.\nThe roles of the columns are defined by the formula.\n\nFor the most part, practitioners’ understanding of what the formula does is dominated by the last purpose. Our focus when typing out a formula is often to declare how the columns should be used. For example, the previous specification we discussed sets up predictors to be used in a specific way:\n(temp + species)^2\nOur focus, when seeing this, is that there are two predictors and the model should contain their main effects and the two-way interactions. However, this formula also implies that, since species is a factor, it should also create indicator variable columns for this predictor (see Section @ref(dummies)) and multiply those columns by the temp column to create the interactions. This transformation represents our second bullet point on encoding; the formula also defines how each column is encoded and can create additional columns that are not in the original data.\n\nThis is an important point that will come up multiple times in this text, especially when we discuss more complex feature engineering in Chapter @ref(recipes) and beyond. The formula in R has some limitations, and our approaches to overcoming them contend with all three aspects."
  },
  {
    "objectID": "03-base-r.html#tidiness-modeling",
    "href": "03-base-r.html#tidiness-modeling",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "3.3 Why Tidiness Is Important for Modeling",
    "text": "3.3 Why Tidiness Is Important for Modeling\nOne of the strengths of R is that it encourages developers to create a user interface that fits their needs. As an example, here are three common methods for creating a scatter plot of two numeric variables in a data frame called plot_data:\n\n\nCode\nplot(plot_data$x, plot_data$y)\n\nlibrary(lattice)\nxyplot(y ~ x, data = plot_data)\n\nlibrary(ggplot2)\nggplot(plot_data, aes(x = x, y = y)) + geom_point()\n\n\nIn these three cases, separate groups of developers devised three distinct interfaces for the same task. Each has advantages and disadvantages.\nIn comparison, the Python Developer’s Guide espouses the notion that, when approaching a problem:\n\n“There should be one – and preferably only one – obvious way to do it.”\n\nR is quite different from Python in this respect. An advantage of R’s diversity of interfaces is that it can evolve over time and fit different needs for different users.\nUnfortunately, some of the syntactical diversity is due to a focus on the needs of the person developing the code instead of the needs of the person using the code. Inconsistencies among packages can be a stumbling block for R users.\nSuppose your modeling project has an outcome with two classes. There are a variety of statistical and machine learning models you could choose from. In order to produce a class probability estimate for each sample, it is common for a model function to have a corresponding predict() method. However, there is significant heterogeneity in the argument values used by those methods to make class probability predictions; this heterogeneity can be difficult for even experienced users to navigate. A sampling of these argument values for different models is shown in Table @ref(tab:probability-args).\n\n\n\nHeterogeneous argument names for different modeling functions.\n \n  \n    Function \n    Package \n    Code \n  \n \n\n  \n    lda() \n    MASS \n    predict(object) \n  \n  \n    glm() \n    stats \n    predict(object, type = \"response\") \n  \n  \n    gbm() \n    gbm \n    predict(object, type = \"response\", n.trees) \n  \n  \n    mda() \n    mda \n    predict(object, type = \"posterior\") \n  \n  \n    rpart() \n    rpart \n    predict(object, type = \"prob\") \n  \n  \n    various \n    RWeka \n    predict(object, type = \"probability\") \n  \n  \n    logitboost() \n    LogitBoost \n    predict(object, type = \"raw\", nIter) \n  \n  \n    pamr.train() \n    pamr \n    pamr.predict(object, type = \"posterior\") \n  \n\n\n\n\nNote that the last example has a custom function to make predictions instead of using the more common predict() interface (the generic predict() method). This lack of consistency is a barrier to day-to-day usage of R for modeling.\nAs another example of unpredictability, the R language has conventions for missing data that are handled inconsistently. The general rule is that missing data propagate more missing data; the average of a set of values with a missing data point is itself missing and so on. When models make predictions, the vast majority require all of the predictors to have complete values. There are several options baked in to R at this point with the generic function na.action(). This sets the policy for how a function should behave if there are missing values. The two most common policies are na.fail() and na.omit(). The former produces an error if missing data are present while the latter removes the missing data prior to calculations by case-wise deletion. From our previous example:\n\n\nCode\n# Add a missing value to the prediction set\nnew_values$temp[1] <- NA\n\n# The predict method for `lm` defaults to `na.pass`:\npredict(main_effect_fit, new_values)\n\n\n       1        2        3        4        5        6 \n      NA 50.43314 54.03589 57.63865 61.24140 64.84415 \n\n\nCode\n# Alternatively \npredict(main_effect_fit, new_values, na.action = na.fail)\n\n\nError in na.fail.default(structure(list(temp = c(NA, 16L, 17L, 18L, 19L, : missing values in object\n\n\nCode\npredict(main_effect_fit, new_values, na.action = na.omit)\n\n\n       2        3        4        5        6 \n50.43314 54.03589 57.63865 61.24140 64.84415 \n\n\nFrom a user’s point of view, na.omit() can be problematic. In our example, new_values has 6 rows but only 5 would be returned with na.omit(). To adjust for this, the user would have to determine which row had the missing value and interleave a missing value in the appropriate place if the predictions were merged into new_values.2 While it is rare that a prediction function uses na.omit() as its missing data policy, this does occur. Users who have determined this as the cause of an error in their code find it quite memorable.\nTo resolve the usage issues described here, the tidymodels packages have a set of design goals. Most of the tidymodels design goals fall under the existing rubric of “Design for Humans” from the tidyverse (Wickham et al. 2019), but with specific applications for modeling code. There are a few additional tidymodels design goals that complement those of the tidyverse. Some examples:\n\nR has excellent capabilities for object-oriented programming, and we use this in lieu of creating new function names (such as a hypothetical new predict_samples() function).\nSensible defaults are very important. Also, functions should have no default for arguments when it is more appropriate to force the user to make a choice (e.g., the file name argument for read_csv()).\nSimilarly, argument values whose default can be derived from the data should be. For example, for glm() the family argument could check the type of data in the outcome and, if no family was given, a default could be determined internally.\nFunctions should take the data structures that users have as opposed to the data structure that developers want. For example, a model function’s only interface should not be constrained to matrices. Frequently, users will have non-numeric predictors such as factors.\n\nMany of these ideas are described in the tidymodels guidelines for model implementation.3 In subsequent chapters, we will illustrate examples of existing issues, along with their solutions.\n\nA few existing R packages provide a unified interface to harmonize these heterogeneous modeling APIs, such as caret and mlr. The tidymodels framework is similar to these in adopting a unification of the function interface, as well as enforcing consistency in the function names and return values. It is different in its opinionated design goals and modeling implementation, discussed in detail throughout this book.\n\nThe broom::tidy() function, which we use throughout this book, is another tool for standardizing the structure of R objects. It can return many types of R objects in a more usable format. For example, suppose that predictors are being screened based on their correlation to the outcome column. Using purrr::map(), the results from cor.test() can be returned in a list for each predictor:\n\n\nCode\ncorr_res <- map(mtcars %>% select(-mpg), cor.test, y = mtcars$mpg)\n\n# The first of ten results in the vector: \ncorr_res[[1]]\n\n\n\n    Pearson's product-moment correlation\n\ndata:  .x[[i]] and mtcars$mpg\nt = -8.9197, df = 30, p-value = 6.113e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9257694 -0.7163171\nsample estimates:\n      cor \n-0.852162 \n\n\nIf we want to use these results in a plot, the standard format of hypothesis test results are not very useful. The tidy() method can return this as a tibble with standardized names:\n\n\nCode\nlibrary(broom)\n\ntidy(corr_res[[1]])\n\n\n\n\n  \n\n\n\nThese results can be “stacked” and added to a ggplot(), as shown in Figure @ref(fig:corr-plot).\n\n\nCode\ncorr_res %>% \n  # Convert each to a tidy format; `map_dfr()` stacks the data frames \n  map_dfr(tidy, .id = \"predictor\") %>% \n  ggplot(aes(x = fct_reorder(predictor, estimate))) + \n  geom_point(aes(y = estimate)) + \n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +\n  labs(x = NULL, y = \"Correlation with mpg\")\n\n\n\n\n\n\n\nCorrelations (and 95% confidence intervals) between predictors and the outcome in the mtcars data set\n\n\n\n\nCreating such a plot is possible using core R language functions, but automatically reformatting the results makes for more concise code with less potential for errors."
  },
  {
    "objectID": "03-base-r.html#combining-base-r-models-and-the-tidyverse",
    "href": "03-base-r.html#combining-base-r-models-and-the-tidyverse",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "3.4 Combining Base R Models and the Tidyverse",
    "text": "3.4 Combining Base R Models and the Tidyverse\nR modeling functions from the core language or other R packages can be used in conjunction with the tidyverse, especially with the dplyr, purrr, and tidyr packages. For example, if we wanted to fit separate models for each cricket species, we can first break out the cricket data by this column using dplyr::group_nest():\n\n\nCode\nsplit_by_species <- \n  crickets %>% \n  group_nest(species) \nsplit_by_species\n\n\n\n\n  \n\n\n\nThe data column contains the rate and temp columns from crickets in a list column. From this, the purrr::map() function can create individual models for each species:\n\n\nCode\nmodel_by_species <- \n  split_by_species %>% \n  mutate(model = map(data, ~ lm(rate ~ temp, data = .x)))\nmodel_by_species\n\n\n\n\n  \n\n\n\nTo collect the coefficients for each of these models, use broom::tidy() to convert them to a consistent data frame format so that they can be unnested:\n\n\nCode\nmodel_by_species %>% \n  mutate(coef = map(model, tidy)) %>% \n  select(species, coef) %>% \n  unnest(cols = c(coef))\n\n\n\n\n  \n\n\n\n\nList columns can be very powerful in modeling projects. List columns provide containers for any type of R objects, from a fitted model itself to the important data frame structure."
  },
  {
    "objectID": "03-base-r.html#the-tidymodels-metapackage",
    "href": "03-base-r.html#the-tidymodels-metapackage",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "3.5 The tidymodels Metapackage",
    "text": "3.5 The tidymodels Metapackage\nThe tidyverse (Chapter @ref(tidyverse)) is designed as a set of modular R packages, each with a fairly narrow scope. The tidymodels framework follows a similar design. For example, the rsample package focuses on data splitting and resampling. Although resampling methods are critical to other activities of modeling (e.g., measuring performance), they reside in a single package, and performance metrics are contained in a different, separate package, yardstick. There are many benefits to adopting this philosophy of modular packages, from less bloated model deployment to smoother package maintenance.\n\n\n\nThe downside to this philosophy is that there are a lot of packages in the tidymodels framework. To compensate for this, the tidymodels package (which you can think of as a metapackage like the tidyverse package) loads a core set of tidymodels and tidyverse packages. Loading the package shows which packages are attached:\n\n\nCode\nlibrary(tidymodels)\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n\n\n✔ broom        1.0.3     ✔ recipes      1.0.4\n✔ dials        1.1.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.0     ✔ tibble       3.1.8\n✔ ggplot2      3.4.0     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.0.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.2\n✔ parsnip      1.0.3     ✔ workflowsets 1.0.0\n✔ purrr        1.0.1     ✔ yardstick    1.1.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\n\nIf you have used the tidyverse, you’ll notice some familiar names as a few tidyverse packages, such as dplyr and ggplot2, are loaded together with the tidymodels packages. We’ve already said that the tidymodels framework applies tidyverse principles to modeling, but the tidymodels framework also literally builds on some of the most fundamental tidyverse packages such as these.\nLoading the metapackage also shows if there are function naming conflicts with previously loaded packages. As an example of a naming conflict, before loading tidymodels, invoking the filter() function will execute the function in the stats package. After loading tidymodels, it will execute the dplyr function of the same name.\nThere are a few ways to handle naming conflicts. The function can be called with its namespace (e.g., stats::filter()). This is not bad practice, but it does make the code less readable.\nAnother option is to use the conflicted package. We can set a rule that remains in effect until the end of the R session to ensure that one specific function will always run if no namespace is given in the code. As an example, if we prefer the dplyr version of the previous function:\n\n\nCode\nlibrary(conflicted)\nconflict_prefer(\"filter\", winner = \"dplyr\")\n\n\nFor convenience, tidymodels contains a function that captures most of the common naming conflicts that we might encounter:\n\n\nCode\ntidymodels_prefer(quiet = FALSE)\n\n\n[conflicted] Will prefer dplyr::filter over any other package.\n[conflicted] Will prefer dplyr::select over any other package.\n[conflicted] Will prefer dplyr::slice over any other package.\n[conflicted] Will prefer dplyr::rename over any other package.\n[conflicted] Will prefer dials::neighbors over any other package.\n[conflicted] Will prefer parsnip::fit over any other package.\n[conflicted] Will prefer parsnip::bart over any other package.\n[conflicted] Will prefer parsnip::pls over any other package.\n[conflicted] Will prefer purrr::map over any other package.\n[conflicted] Will prefer recipes::step over any other package.\n[conflicted] Will prefer themis::step_downsample over any other package.\n[conflicted] Will prefer themis::step_upsample over any other package.\n[conflicted] Will prefer tune::tune over any other package.\n[conflicted] Will prefer yardstick::precision over any other package.\n[conflicted] Will prefer yardstick::recall over any other package.\n[conflicted] Will prefer yardstick::spec over any other package.\n── Conflicts ──────────────────────────────────────────── tidymodels_prefer() ──\n\n\n\nBe aware that using this function opts you in to using conflicted::conflict_prefer() for all namespace conflicts, making every conflict an error and forcing you to choose which function to use. The function tidymodels::tidymodels_prefer() handles the most common conflicts from tidymodels functions, but you will need to handle other conflicts in your R session yourself."
  },
  {
    "objectID": "03-base-r.html#chapter-summary",
    "href": "03-base-r.html#chapter-summary",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "3.6 Chapter Summary",
    "text": "3.6 Chapter Summary\nThis chapter reviewed core R language conventions for creating and using models that are an important foundation for the rest of this book. The formula operator is an expressive and important aspect of fitting models in R and often serves multiple purposes in non-tidymodels functions. Traditional R approaches to modeling have some limitations, especially when it comes to fluently handling and visualizing model output. The tidymodels metapackage applies tidyverse design philosophy to modeling packages.\n\n\n\n\nChambers, J, and T Hastie, eds. 1992. Statistical Models in S. Boca Raton, FL: CRC Press, Inc.\n\n\nMangiafico, S. 2015. “An R Companion for the Handbook of Biological Statistics.” https://rcompanion.org/handbook/.\n\n\nMcDonald, J. 2009. Handbook of Biological Statistics. Sparky House Publishing.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43)."
  },
  {
    "objectID": "04-ames.html#exploring-features-of-homes-in-ames",
    "href": "04-ames.html#exploring-features-of-homes-in-ames",
    "title": "4  The Ames Housing Data",
    "section": "4.1 Exploring Features of Homes in Ames",
    "text": "4.1 Exploring Features of Homes in Ames\nLet’s start our exploratory data analysis by focusing on the outcome we want to predict: the last sale price of the house (in USD). We can create a histogram to see the distribution of sale prices in Figure @ref(fig:ames-sale-price-hist).\n\n\nCode\nlibrary(tidymodels)\ntidymodels_prefer()\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n\n\n\n\n\n\n\nSale prices of houses in Ames, Iowa\n\n\n\n\nThis plot shows us that the data are right-skewed; there are more inexpensive houses than expensive ones. The median sale price was $160,000, and the most expensive house was $755,000. When modeling this outcome, a strong argument can be made that the price should be log-transformed. The advantages of this type of transformation are that no houses would be predicted with negative sale prices and that errors in predicting expensive houses will not have an undue influence on the model. Also, from a statistical perspective, a logarithmic transform may also stabilize the variance in a way that makes inference more legitimate. We can use similar steps to now visualize the transformed data, shown in Figure @ref(fig:ames-log-sale-price-hist).\n\n\nCode\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\") +\n  scale_x_log10()\n\n\n\n\n\n\n\nSale prices of houses in Ames, Iowa after a log (base 10) transformation\n\n\n\n\nWhile not perfect, this will likely result in better models than using the untransformed data, for the reasons just outlined.\n\nThe disadvantages of transforming the outcome mostly relate to interpretation of model results.\n\nThe units of the model coefficients might be more difficult to interpret, as will measures of performance. For example, the root mean squared error (RMSE) is a common performance metric used in regression models. It uses the difference between the observed and predicted values in its calculations. If the sale price is on the log scale, these differences (i.e., the residuals) are also on the log scale. It can be difficult to understand the quality of a model whose RMSE is 0.15 on such a log scale.\nDespite these drawbacks, the models used in this book use the log transformation for this outcome. From this point on, the outcome column is prelogged in the ames data frame:\n\n\nCode\names <- ames %>% mutate(Sale_Price = log10(Sale_Price))\n\n\nAnother important aspect of these data for our modeling is their geographic locations. This spatial information is contained in the data in two ways: a qualitative Neighborhood label as well as quantitative longitude and latitude data. To visualize the spatial information, let’s use both together to plot the data on a map in Figure @ref(fig:ames-map).\n\n\n\n\n\nNeighborhoods in Ames, IA\n\n\n\n\nWe can see a few noticeable patterns. First, there is a void of data points in the center of Ames. This corresponds to the campus of Iowa State University where there are no residential houses. Second, while there are a number of adjacent neighborhoods, others are geographically isolated. For example, as Figure @ref(fig:ames-timberland) shows, Timberland is located apart from almost all other neighborhoods.\n\n\n\n\n\nLocations of homes in Timberland\n\n\n\n\nFigure @ref(fig:ames-mitchell) visualizes how the Meadow Village neighborhood in southwest Ames is like an island of properties inside the sea of properties that make up the Mitchell neighborhood.\n\n\n\n\n\nLocations of homes in Meadow Village and Mitchell\n\n\n\n\nA detailed inspection of the map also shows that the neighborhood labels are not completely reliable. For example, Figure @ref(fig:ames-northridge) shows some properties labeled as being in Northridge are surrounded by homes in the adjacent Somerset neighborhood.\n\n\n\n\n\nLocations of homes in Somerset and Northridge\n\n\n\n\nAlso, there are ten isolated homes labeled as being in Crawford that, as you can see in Figure @ref(fig:ames-crawford), are not close to the majority of the other homes in that neighborhood.\n\n\n\n\n\nLocations of homes in Crawford\n\n\n\n\nAlso notable is the “Iowa Department of Transportation (DOT) and Rail Road” neighborhood adjacent to the main road on the east side of Ames, shown in Figure @ref(fig:ames-dot-rr). There are several clusters of homes within this neighborhood as well as some longitudinal outliers; the two homes farthest east are isolated from the other locations.\n\n\n\n\n\nHomes labeled as Iowa Department of Transportation (DOT) and Rail Road\n\n\n\n\nAs described in Chapter @ref(software-modeling), it is critical to conduct exploratory data analysis prior to beginning any modeling. These housing data have characteristics that present interesting challenges about how the data should be processed and modeled. We describe many of these in later chapters. Some basic questions that could be examined during this exploratory stage include:\n\nIs there anything odd or noticeable about the distributions of the individual predictors? Is there much skewness or any pathological distributions?\nAre there high correlations between predictors? For example, there are multiple predictors related to house size. Are some redundant?\nAre there associations between predictors and the outcomes?\n\nMany of these questions will be revisited as these data are used throughout this book."
  },
  {
    "objectID": "04-ames.html#ames-summary",
    "href": "04-ames.html#ames-summary",
    "title": "4  The Ames Housing Data",
    "section": "4.2 Chapter Summary",
    "text": "4.2 Chapter Summary\nThis chapter introduced the Ames housing data set and investigated some of its characteristics. This data set will be used in later chapters to demonstrate tidymodels syntax. Exploratory data analysis like this is an essential component of any modeling project; EDA uncovers information that contributes to better modeling practice.\nThe important code for preparing the Ames data set that we will carry forward into subsequent chapters is:\n\n\nCode\nlibrary(tidymodels)\ndata(ames)\names <- ames %>% mutate(Sale_Price = log10(Sale_Price))\n\n\n\n\n\n\nDe Cock, D. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3)."
  },
  {
    "objectID": "04-ames.html",
    "href": "04-ames.html",
    "title": "Modeling Basics",
    "section": "",
    "text": "The Ames Housing Data\nIn this chapter, we’ll introduce the Ames housing data set (De Cock 2011), which we will use in modeling examples throughout this book. Exploratory data analysis, like what we walk through in this chapter, is an important first step in building a reliable model. The data set contains information on 2,930 properties in Ames, Iowa, including columns related to:\nThe raw housing data are provided in De Cock (2011), but in our analyses in this book, we use a transformed version available in the modeldata package. This version has several changes and improvements to the data.1 For example, the longitude and latitude values have been determined for each property. Also, some columns were modified to be more analysis ready. For example:\nTo load the data:"
  },
  {
    "objectID": "05-data-spending.html#splitting-methods",
    "href": "05-data-spending.html#splitting-methods",
    "title": "5  Spending our Data",
    "section": "5.1 Common Methods for Splitting Data",
    "text": "5.1 Common Methods for Splitting Data\nThe primary approach for empirical model validation is to split the existing pool of data into two distinct sets, the training set and the test set. One portion of the data is used to develop and optimize the model. This training set is usually the majority of the data. These data are a sandbox for model building where different models can be fit, feature engineering strategies are investigated, and so on. As modeling practitioners, we spend the vast majority of the modeling process using the training set as the substrate to develop the model.\nThe other portion of the data is placed into the test set. This is held in reserve until one or two models are chosen as the methods most likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. It is critical to look at the test set only once; otherwise, it becomes part of the modeling process.\n\nHow should we conduct this split of the data? The answer depends on the context.\n\nSuppose we allocate 80% of the data to the training set and the remaining 20% for testing. The most common method is to use simple random sampling. The rsample package has tools for making data splits such as this; the function initial_split() was created for this purpose. It takes the data frame as an argument as well as the proportion to be placed into training. Using the data frame produced by the code snippet from the summary in Section @ref(ames-summary) that prepared the Ames data set:\n\n\nCode\nlibrary(tidymodels)\ntidymodels_prefer()\n\n# Set the random number stream using `set.seed()` so that the results can be \n# reproduced later. \nset.seed(501)\n\n# Save the split information for an 80/20 split of the data\names_split <- initial_split(ames, prop = 0.80)\names_split\n\n\n<Training/Testing/Total>\n<2344/586/2930>\n\n\nThe printed information denotes the amount of data in the training set (\\(n = 2,344\\)), the amount in the test set (\\(n = 586\\)), and the size of the original pool of samples (\\(n = 2,930\\)).\nThe object ames_split is an rsplit object and contains only the partitioning information; to get the resulting data sets, we apply two more functions:\n\n\nCode\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\ndim(ames_train)\n\n\n[1] 2344   74\n\n\nThese objects are data frames with the same columns as the original data but only the appropriate rows for each set.\nSimple random sampling is appropriate in many cases but there are exceptions. When there is a dramatic class imbalance in classification problems, one class occurs much less frequently than another. Using a simple random sample may haphazardly allocate these infrequent samples disproportionately into the training or test set. To avoid this, stratified sampling can be used. The training/test split is conducted separately within each class and then these subsamples are combined into the overall training and test set. For regression problems, the outcome data can be artificially binned into quartiles and then stratified sampling can be conducted four separate times. This is an effective method for keeping the distributions of the outcome similar between the training and test set. The distribution of the sale price outcome for the Ames housing data is shown in Figure @ref(fig:ames-sale-price).\n\n\n\n\n\nThe distribution of the sale price (in log units) for the Ames housing data. The vertical lines indicate the quartiles of the data\n\n\n\n\nAs discussed in Chapter @ref(ames), the sale price distribution is right-skewed, with proportionally more inexpensive houses than expensive houses on either side of the center of the distribution. The worry here with simple splitting is that the more expensive houses would not be well represented in the training set; this would increase the risk that our model would be ineffective at predicting the price for such properties. The dotted vertical lines in Figure @ref(fig:ames-sale-price) indicate the four quartiles for these data. A stratified random sample would conduct the 80/20 split within each of these data subsets and then pool the results. In rsample, this is achieved using the strata argument:\n\n\nCode\nset.seed(502)\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\ndim(ames_train)\n\n\n[1] 2342   74\n\n\nOnly a single column can be used for stratification.\n\nThere is very little downside to using stratified sampling.\n\nAre there situations when random sampling is not the best choice? One case is when the data have a significant time component, such as time series data. Here, it is more common to use the most recent data as the test set. The rsample package contains a function called initial_time_split() that is very similar to initial_split(). Instead of using random sampling, the prop argument denotes what proportion of the first part of the data should be used as the training set; the function assumes that the data have been pre-sorted in an appropriate order.\n\nThe proportion of data that should be allocated for splitting is highly dependent on the context of the problem at hand. Too little data in the training set hampers the model’s ability to find appropriate parameter estimates. Conversely, too little data in the test set lowers the quality of the performance estimates. Parts of the statistics community eschew test sets in general because they believe all of the data should be used for parameter estimation. While there is merit to this argument, it is good modeling practice to have an unbiased set of observations as the final arbiter of model quality. A test set should be avoided only when the data are pathologically small."
  },
  {
    "objectID": "05-data-spending.html#what-about-a-validation-set",
    "href": "05-data-spending.html#what-about-a-validation-set",
    "title": "5  Spending our Data",
    "section": "5.2 What About a Validation Set?",
    "text": "5.2 What About a Validation Set?\nWhen describing the goals of data splitting, we singled out the test set as the data that should be used to properly evaluate of model performance on the final model(s). This begs the question: “How can we tell what is best if we don’t measure performance until the test set?”\nIt is common to hear about validation sets as an answer to this question, especially in the neural network and deep learning literature. During the early days of neural networks, researchers realized that measuring performance by re-predicting the training set samples led to results that were overly optimistic (significantly, unrealistically so). This led to models that overfit, meaning that they performed very well on the training set but poorly on the test set.1 To combat this issue, a small validation set of data were held back and used to measure performance as the network was trained. Once the validation set error rate began to rise, the training would be halted. In other words, the validation set was a means to get a rough sense of how well the model performed prior to the test set.\n\nWhether validation sets are a subset of the training set or a third allocation in the initial split of the data largely comes down to semantics.\n\nValidation sets are discussed more in Section @ref(validation) as a special case of resampling methods that are used on the training set."
  },
  {
    "objectID": "05-data-spending.html#multilevel-data",
    "href": "05-data-spending.html#multilevel-data",
    "title": "5  Spending our Data",
    "section": "5.3 Multilevel Data",
    "text": "5.3 Multilevel Data\nWith the Ames housing data, a property is considered to be the independent experimental unit. It is safe to assume that, statistically, the data from a property are independent of other properties. For other applications, that is not always the case:\n\nFor longitudinal data, for example, the same independent experimental unit can be measured over multiple time points. An example would be a human subject in a medical trial.\nA batch of manufactured product might also be considered the independent experimental unit. In repeated measures designs, replicate data points from a batch are collected at multiple times.\nJohnson et al. (2018) report an experiment where different trees were sampled across the top and bottom portions of a stem. Here, the tree is the experimental unit and the data hierarchy is sample within stem position within tree.\n\nChapter 9 of Kuhn and Johnson (2020) contains other examples.\nIn these situations, the data set will have multiple rows per experimental unit. Simple resampling across rows would lead to some data within an experimental unit being in the training set and others in the test set. Data splitting should occur at the independent experimental unit level of the data. For example, to produce an 80/20 split of the Ames housing data set, 80% of the properties should be allocated for the training set."
  },
  {
    "objectID": "05-data-spending.html#other-considerations-for-a-data-budget",
    "href": "05-data-spending.html#other-considerations-for-a-data-budget",
    "title": "5  Spending our Data",
    "section": "5.4 Other Considerations for a Data Budget",
    "text": "5.4 Other Considerations for a Data Budget\nWhen deciding how to spend the data available to you, keep a few more things in mind. First, it is critical to quarantine the test set from any model building activities. As you read this book, notice which data are exposed to the model at any given time.\n\nThe problem of information leakage occurs when data outside of the training set are used in the modeling process.\n\nFor example, in a machine learning competition, the test set data might be provided without the true outcome values so that the model can be scored and ranked. One potential method for improving the score might be to fit the model using the training set points that are most similar to the test set values. While the test set isn’t directly used to fit the model, it still has a heavy influence. In general, this technique is highly problematic since it reduces the generalization error of the model to optimize performance on a specific data set. There are more subtle ways that the test set data can be used during training. Keeping the training data in a separate data frame from the test set is one small check to make sure that information leakage does not occur by accident.\nSecond, techniques to subsample the training set can mitigate specific issues (e.g., class imbalances). This is a valid and common technique that deliberately results in the training set data diverging from the population from which the data were drawn. It is critical that the test set continues to mirror what the model would encounter in the wild. In other words, the test set should always resemble new data that will be given to the model.\nNext, at the beginning of this chapter, we warned about using the same data for different tasks. Chapter @ref(resampling) will discuss solid, data-driven methodologies for data usage that will reduce the risks related to bias, overfitting, and other issues. Many of these methods apply the data-splitting tools introduced in this chapter.\nFinally, the considerations in this chapter apply to developing and choosing a reliable model, the main topic of this book. When training a final chosen model for production, after ascertaining the expected performance on new data, practitioners often use all available data for better parameter estimation."
  },
  {
    "objectID": "05-data-spending.html#splitting-summary",
    "href": "05-data-spending.html#splitting-summary",
    "title": "5  Spending our Data",
    "section": "5.5 Chapter Summary",
    "text": "5.5 Chapter Summary\nData splitting is the fundamental strategy for empirical validation of models. Even in the era of unrestrained data collection, a typical modeling project has a limited amount of appropriate data, and wise spending of a project’s data is necessary. In this chapter, we discussed several strategies for partitioning the data into distinct groups for modeling and evaluation.\nAt this checkpoint, the important code snippets for preparing and splitting are:\n\n\nCode\nlibrary(tidymodels)\ndata(ames)\names <- ames %>% mutate(Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\n\n\n\n\n\nJohnson, D, P Eckart, N Alsamadisi, H Noble, C Martin, and R Spicer. 2018. “Polar Auxin Transport Is Implicated in Vessel Differentiation and Spatial Patterning During Secondary Growth in Populus.” American Journal of Botany 105 (2): 186–96.\n\n\nKuhn, M, and K Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press."
  }
]